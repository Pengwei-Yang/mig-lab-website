<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://pengwei-yang.github.io/mig-lab-website/feed.xml" rel="self" type="application/atom+xml" /><link href="https://pengwei-yang.github.io/mig-lab-website/" rel="alternate" type="text/html" /><updated>2025-07-29T01:54:14+00:00</updated><id>https://pengwei-yang.github.io/mig-lab-website/feed.xml</id><title type="html">Multi-modal Intelligence Group</title><subtitle>An engaging 1-3 sentence description of your lab.</subtitle><entry><title type="html"></title><link href="https://pengwei-yang.github.io/mig-lab-website/2025/07/29/2025-06-25-new-publication.html" rel="alternate" type="text/html" title="" /><published>2025-07-29T01:54:14+00:00</published><updated>2025-07-29T01:52:24+00:00</updated><id>https://pengwei-yang.github.io/mig-lab-website/2025/07/29/2025-06-25-new-publication</id><content type="html" xml:base="https://pengwei-yang.github.io/mig-lab-website/2025/07/29/2025-06-25-new-publication.html"><![CDATA[<p>Recently, the paper “Implicit Counterfactual Learning for Audio-Visual Segmentation” submitted by MIG member Mingfeng Zha was accepted by ICCV2025. Congratulations to Mingfeng Zha and all the co-authors!!!</p>]]></content><author><name></name></author></entry></feed>